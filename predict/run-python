#!/bin/bash

start_model_serving_python () {
    echo "PIPELINE_NAME=$PIPELINE_NAME"
    echo "PIPELINE_TAG=$PIPELINE_TAG"
    echo "PIPELINE_RUNTIME=$PIPELINE_RUNTIME"
    echo "PIPELINE_CHIP=$PIPELINE_CHIP"
    echo "PIPELINE_RESOURCE_TYPE=$PIPELINE_RESOURCE_TYPE"
    echo "PIPELINE_RESOURCE_SUBTYPE=$PIPELINE_RESOURCE_SUBTYPE"
    echo "PIPELINE_RESOURCE_NAME=$PIPELINE_RESOURCE_NAME"
    echo "PIPELINE_RESOURCE_TAG=$PIPELINE_RESOURCE_TAG"
    echo "PIPELINE_RESOURCE_PATH=$PIPELINE_RESOURCE_PATH"
    echo "PIPELINE_RESOURCE_SERVER_PATH=$PIPELINE_RESOURCE_SERVER_PATH"
    echo "PIPELINE_RESOURCE_SERVER_PORT=$PIPELINE_RESOURCE_SERVER_PORT"
    echo "PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT"

    echo ""
    echo "Starting Python-based Model Serving..."
    echo ""

    source activate $PIPELINE_RESOURCE_PREDICT_CONDA_ENV_NAME

    # cd here to preserve the model's natural './' paths`
    cd $PIPELINE_RESOURCE_PATH 

    PYTHONPATH=$PIPELINE_RESOURCE_PATH:$PIPELINE_RESOURCE_SERVER_PATH:$PYTHONPATH \
        $PIPELINE_RESOURCE_SERVER_PATH/model_server_python.py \
        --PIPELINE_NAME=$PIPELINE_NAME \
        --PIPELINE_TAG=$PIPELINE_TAG \
        --PIPELINE_RUNTIME=$PIPELINE_RUNTIME \
        --PIPELINE_CHIP=$PIPELINE_CHIP \
        --PIPELINE_RESOURCE_TYPE=$PIPELINE_RESOURCE_TYPE \
        --PIPELINE_RESOURCE_SUBTYPE=$PIPELINE_RESOURCE_SUBTYPE \
        --PIPELINE_RESOURCE_NAME=$PIPELINE_RESOURCE_NAME \
        --PIPELINE_RESOURCE_TAG=$PIPELINE_RESOURCE_TAG \
        --PIPELINE_RESOURCE_PATH=$PIPELINE_RESOURCE_PATH \
        --PIPELINE_RESOURCE_SERVER_PORT=$PIPELINE_RESOURCE_SERVER_PORT \
        --PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_RESOURCE_SERVER_TENSORFLOW_SERVING_PORT
}

echo ""
echo "___________________________________________"
echo " __     __   ___              ___          "
echo "|__) | |__) |__  |    | |\ | |__      /\  |"
echo "|    | |    |___ |___ | | \| |___    /~~\ |"
echo "___________________________________________"
echo ""

source activate $PIPELINE_RESOURCE_PREDICT_CONDA_ENV_NAME

source /root/sysutils/container-limits.sh

# Start Nginx Server
service nginx start

echo "Required Environment Variables..."
echo "PIPELINE_NAME=$PIPELINE_NAME"
echo "PIPELINE_TAG=$PIPELINE_TAG"
echo "PIPELINE_RUNTIME=$PIPELINE_RUNTIME"
echo "PIPELINE_CHIP=$PIPELINE_CHIP"
echo "PIPELINE_RESOURCE_NAME=$PIPELINE_RESOURCE_NAME"
echo "PIPELINE_RESOURCE_TAG=$PIPELINE_RESOURCE_TAG"
echo "PIPELINE_RESOURCE_TYPE=$PIPELINE_RESOURCE_TYPE"
echo "PIPELINE_RESOURCE_SUBTYPE=$PIPELINE_RESOURCE_SUBTYPE"
echo "PIPELINE_RESOURCE_PATH=$PIPELINE_RESOURCE_PATH"
echo "PIPELINE_SINGLE_SERVER_ONLY=$PIPELINE_SINGLE_SERVER_ONLY"
echo "PIPELINE_ENABLE_STREAM_PREDICTIONS=$PIPELINE_ENABLE_STREAM_PREDICTIONS"

export

echo "Current working directory..."
cd $PIPELINE_RESOURCE_PATH
ls -l .

##########################################
# GPU-Only
[ "$PIPELINE_CHIP" == "gpu" ] && cp /rootfs/usr/lib/x86_64-linux-gnu/libcuda.* /usr/lib/x86_64-linux-gnu/
#[ "$PIPELINE_CHIP" == "gpu" ] && cp /rootfs/usr/lib/x86_64-linux-gnu/libcudnn.* /usr/lib/x86_64-linux-gnu/
[ "$PIPELINE_CHIP" == "gpu" ] && cp /rootfs/usr/lib/x86_64-linux-gnu/libnvidia* /usr/lib/x86_64-linux-gnu/
##########################################

# Start Tornado/Python-based Model Server
start_model_serving_python 
